{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Environment Setup\n",
    "\n",
    " **Kaggle Environment Requirements**:\n",
    " - Python 3.11.11\n",
    " - Key Packages:\n",
    "   - tensorflow==2.18.0\n",
    "   - antspynet==0.3.0 (installed from GitHub)\n",
    "   - nibabel==5.3.2\n",
    "   - plotly==5.24.1\n",
    "   - scipy==1.15.2\n",
    "   - scikit-learn\n",
    "   - statsmodels\n",
    "\n",
    " **Installation Command**:\n",
    " ```\n",
    " %pip install git+https://github.com/ValV/ANTsPyNet.git\n",
    " %pip install tensorflow==2.18.0 nibabel plotly scipy scikit-learn statsmodels\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Dependencies\n",
    "\n",
    " Install required packages including ANTsPyNet from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install git+https://github.com/ValV/ANTsPyNet.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Processing\n",
    "\n",
    " ## Utility Functions\n",
    "\n",
    " Functions for processing MRI data and extracting surface coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os import path as osp\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "from scipy.ndimage import binary_erosion\n",
    "\n",
    "\n",
    "def get_surface_coords(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns coordinates of surface voxels of a binary 3D mask\n",
    "\n",
    "    Args:\n",
    "        mask: 3D binary numpy array representing a brain mask\n",
    "\n",
    "    Returns:\n",
    "        coords: Array of (x,y,z) coordinates for surface voxels\n",
    "    \"\"\"\n",
    "    eroded = binary_erosion(mask)\n",
    "    surface = mask ^ eroded  # XOR operation to get surface voxels\n",
    "    coords = np.argwhere(surface)\n",
    "    return coords\n",
    "\n",
    "\n",
    "def downsample(coords: np.ndarray, factor: int = 16) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Downsample coordinate array by specified factor\n",
    "\n",
    "    Args:\n",
    "        coords: Array of 3D coordinates\n",
    "        factor: Downsampling factor (keep every nth point)\n",
    "\n",
    "    Returns:\n",
    "        Downsampled coordinate array\n",
    "    \"\"\"\n",
    "    return coords[::factor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data Loading\n",
    "\n",
    " Load sample MRI scans from CONTROLS and PATIENTS directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PREFIX = osp.join('/', 'kaggle', 'input', 'brainsearch-classification')\n",
    "index = 4\n",
    "\n",
    "# Load MRI scans\n",
    "path_controls = glob(osp.join(PATH_PREFIX, 'CONTROLS', '*.nii'))\n",
    "path_patients = glob(osp.join(PATH_PREFIX, 'PATIENTS', '*.nii'))\n",
    "\n",
    "source_controls = nib.load(path_controls[index])\n",
    "source_patients = nib.load(path_patients[index])\n",
    "\n",
    "# Get image data as numpy arrays\n",
    "image_control = source_controls.get_fdata()\n",
    "image_patient = source_patients.get_fdata()\n",
    "\n",
    "# Extract surface coordinates\n",
    "coords_control = get_surface_coords(image_control > 0)\n",
    "coords_patient = get_surface_coords(image_patient > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Interactive 3D Visualization\n",
    "\n",
    " Visualize brain surface using Plotly (requires Plotly installation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def scatter3d(coords: np.ndarray, color: str, name: str) -> go.Scatter3d:\n",
    "    \"\"\"\n",
    "    Create 3D scatter plot for brain surface visualization\n",
    "\n",
    "    Args:\n",
    "        coords: Array of 3D coordinates\n",
    "        color: Color for the points\n",
    "        name: Name for the plot legend\n",
    "\n",
    "    Returns:\n",
    "        Plotly Scatter3d object\n",
    "    \"\"\"\n",
    "    return go.Scatter3d(\n",
    "        x=coords[:, 0],\n",
    "        y=coords[:, 1],\n",
    "        z=coords[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=2, color=color),\n",
    "        name=name,\n",
    "        hoverinfo='skip',\n",
    "    )\n",
    "\n",
    "\n",
    "# Create visualization figure\n",
    "figure = go.Figure(\n",
    "    data=[scatter3d(downsample(coords_control, 24), 'gray', 'Control')]\n",
    ")\n",
    "\n",
    "figure.update_layout(\n",
    "    title='3D Scatter Plot of a Brain (Surface Voxels)',\n",
    "    scene=dict(\n",
    "        xaxis=dict(visible=False, showbackground=False),\n",
    "        yaxis=dict(visible=False, showbackground=False),\n",
    "        zaxis=dict(visible=False, showbackground=False),\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=40),\n",
    ")\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Multi-view Visualization\n",
    "\n",
    " Show sagittal, axial, and coronal views using Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_angle(ax, coords: np.ndarray, color: str, label: str):\n",
    "    \"\"\"\n",
    "    Plot brain surface coordinates in 3D from specific angle\n",
    "\n",
    "    Args:\n",
    "        ax: Matplotlib axis object\n",
    "        coords: Array of 3D coordinates\n",
    "        color: Color for the points\n",
    "        label: Label for the legend\n",
    "    \"\"\"\n",
    "    ax.scatter(*(coords).T[::-1], s=1, c=color, label=label)\n",
    "\n",
    "\n",
    "# Create multi-view visualization\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "angles = ((90, -180), (0, 0), (0, 90))  # (elevation, azimuth)\n",
    "views = ('Sagittal', 'Axial', 'Coronal')\n",
    "\n",
    "for i, (elevation, azimuth) in enumerate(angles):\n",
    "    ax = fig.add_subplot(1, 3, i + 1, projection='3d')\n",
    "    plot_angle(ax, downsample(coords_patient, 12), 'yellow', 'Patient')\n",
    "    plot_angle(ax, downsample(coords_control, 24), 'gray', 'Control')\n",
    "    ax.view_init(elev=elevation, azim=azimuth)\n",
    "    ax.set_title(views[i])\n",
    "    ax.set_xlim(0, 200)\n",
    "    ax.set_ylim(0, 200)\n",
    "    ax.set_zlim(0, 150)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Dataset Preparation\n",
    "\n",
    " Custom dataset class for loading and preprocessing MRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import ants\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "TARGET_ORIENTATION = 'RAS'\n",
    "TEMPLATE_PATH = './MNI152_T1_1mm.nii.gz'\n",
    "TEMPLATE_URL = 'https://github.com/Jfortin1/MNITemplate/raw/master/inst/extdata/MNI152_T1_1mm.nii.gz'\n",
    "\n",
    "\n",
    "class BrainMRIDataset:\n",
    "    \"\"\"\n",
    "    Custom dataset for loading and preprocessing brain MRI scans\n",
    "\n",
    "    Key Features:\n",
    "    - Handles both control and patient data\n",
    "    - Performs spatial normalization to MNI152 template\n",
    "    - Supports template-based registration\n",
    "    - Generates TensorFlow datasets\n",
    "\n",
    "    Args:\n",
    "        path_prefix: Root directory containing 'CONTROLS' and 'PATIENTS' folders\n",
    "        use_template: Whether to use template-based normalization (bool or path to custom template)\n",
    "        shuffle: Whether to shuffle samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        path_prefix: str,\n",
    "        use_template: bool | str = False,\n",
    "        shuffle: bool = False,\n",
    "    ):\n",
    "        self.shapes = set()\n",
    "        self.samples = []\n",
    "        self.label_map = {}\n",
    "\n",
    "        # Load MNI152 template if needed\n",
    "        if not osp.isfile(TEMPLATE_PATH):\n",
    "            urlretrieve(TEMPLATE_URL, TEMPLATE_PATH)\n",
    "        self.template = ants.image_read(TEMPLATE_PATH)\n",
    "\n",
    "        # Configure normalization\n",
    "        self.normalize = False\n",
    "        if isinstance(use_template, bool):\n",
    "            self.normalize = use_template\n",
    "        elif osp.isfile(use_template):\n",
    "            self.template = ants.image_read(use_template)\n",
    "            self.normalize = True\n",
    "\n",
    "        # Load and preprocess data\n",
    "        data = []\n",
    "        for idx, label in enumerate(sorted(ls(path_prefix))):\n",
    "            if not osp.isdir(osp.join(path_prefix, label)):\n",
    "                continue\n",
    "            path_class = osp.join(path_prefix, label)\n",
    "            self.label_map[idx] = label\n",
    "            data.append((path_class, idx))\n",
    "\n",
    "        # Process all MRI files\n",
    "        c = 0\n",
    "        for path_class, idx in data:\n",
    "            for i, nii in enumerate(glob(osp.join(path_class, '*.nii'))):\n",
    "                array, label = self._preprocess(nii, idx)\n",
    "                self.shapes.add(array.shape)\n",
    "                self.samples.append((array, label))\n",
    "                print(f\"{i + 1:03d}:{c + 1:03d} Loaded '{nii}'\")\n",
    "                c += 1\n",
    "\n",
    "        # Handle dataset shuffling\n",
    "        if shuffle:\n",
    "            random.shuffle(self.samples)\n",
    "\n",
    "        self.num_classes = len(self.label_map)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def _preprocess(self, path: str, label: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Preprocess MRI scan:\n",
    "        1. Reorient to RAS coordinate system\n",
    "        2. Perform template-based normalization (if enabled)\n",
    "        3. Intensity normalization (z-score)\n",
    "\n",
    "        Args:\n",
    "            path: Path to .nii file\n",
    "            label: Class label (0=control, 1=patient)\n",
    "\n",
    "        Returns:\n",
    "            tuple: (preprocessed_array, label)\n",
    "        \"\"\"\n",
    "        # Load NIfTI file\n",
    "        img = nib.load(path)\n",
    "        data = img.get_fdata()\n",
    "\n",
    "        # Reorient to RAS coordinate system\n",
    "        orient = nib.orientations.io_orientation(img.affine)\n",
    "        transform = nib.orientations.ornt_transform(\n",
    "            orient, nib.orientations.axcodes2ornt(TARGET_ORIENTATION)\n",
    "        )\n",
    "        data = nib.orientations.apply_orientation(data, transform)\n",
    "        affine = img.affine.dot(\n",
    "            nib.orientations.inv_ornt_aff(transform, img.shape)\n",
    "        )\n",
    "\n",
    "        # ANTs-based processing\n",
    "        itk = ants.from_numpy(data, spacing=img.header.get_zooms())\n",
    "        itk.set_origin(list(affine[:3, 3]))\n",
    "\n",
    "        # Template-based normalization\n",
    "        if self.normalize and self.template is not None:\n",
    "            res = ants.registration(\n",
    "                fixed=self.template, moving=itk, type_of_transform='Affine'\n",
    "            )\n",
    "            warped = ants.resample_image(\n",
    "                res['warpedmovout'],\n",
    "                self.template.shape,\n",
    "                use_voxels=True,\n",
    "                interp_type=0,\n",
    "            )\n",
    "            arr = warped.numpy()\n",
    "        else:\n",
    "            arr = itk.numpy()\n",
    "\n",
    "        # Intensity normalization (z-score)\n",
    "        arr = (arr - arr.mean()) / (arr.std() + 1e-6)\n",
    "        arr = np.expand_dims(\n",
    "            arr, axis=-1\n",
    "        )  # Add channel dimension: (D, H, W, 1)\n",
    "\n",
    "        return arr, label\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple:\n",
    "        return self.samples[index]\n",
    "\n",
    "    def generator(self):\n",
    "        \"\"\"Generator function for tf.data API\"\"\"\n",
    "        for i in range(len(self)):\n",
    "            yield self[i]\n",
    "\n",
    "\n",
    "def build_dataset(\n",
    "    path_root: str,\n",
    "    batch_size: int = 2,\n",
    "    use_template: bool = False,\n",
    "    shuffle: bool = True,\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Build TensorFlow dataset from brain MRI data\n",
    "\n",
    "    Args:\n",
    "        path_root: Root directory containing MRI data\n",
    "        batch_size: Batch size for training\n",
    "        use_template: Whether to use template normalization\n",
    "        shuffle: Whether to shuffle data\n",
    "\n",
    "    Returns:\n",
    "        tuple: (tf.data.Dataset, BrainMRIDataset instance)\n",
    "    \"\"\"\n",
    "    brains = BrainMRIDataset(\n",
    "        path_root, use_template=use_template, shuffle=shuffle\n",
    "    )\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        brains.generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=brains.shape, dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "        ),\n",
    "    )\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=64)\n",
    "    dataset = dataset.batch(batch_size).prefetch(AUTOTUNE)\n",
    "    return dataset, brains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Model Definition\n",
    "\n",
    " Create 3D ResNet model using ANTsPyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import antspynet\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    num_classes: int,\n",
    "    shape: tuple = (128, 128, 128, 1),\n",
    "    learning_rate: float = 1e-4,\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Build 3D ResNet model for classification\n",
    "\n",
    "    Architecture Features:\n",
    "    - 3D convolutional neural network\n",
    "    - Residual connections\n",
    "    - Squeeze-and-excitation blocks\n",
    "    - Output layer with softmax activation\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes (2 for control/patient)\n",
    "        shape: Input volume shape (depth, height, width, channels)\n",
    "        learning_rate: Learning rate for Adam optimizer\n",
    "\n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = antspynet.create_resnet_model_3d(\n",
    "        input_image_size=shape,\n",
    "        number_of_outputs=num_classes,\n",
    "        layers=(2, 2, 2, 2),  # Number of blocks in each stage\n",
    "        lowest_resolution=32,  # Number of filters in first convolution\n",
    "        mode='classification',\n",
    "        squeeze_and_excite=True,  # Use channel attention mechanism\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Training Setup\n",
    "\n",
    " Configure training parameters and data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Configuration\n",
    "root = osp.join('/', 'kaggle', 'input', 'brainsearch-classification')\n",
    "batch_size = 2\n",
    "epochs = 20\n",
    "\n",
    "# Create log directory\n",
    "path_logs = osp.join('runs', datetime.now().strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "callback_tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=path_logs,\n",
    "    update_freq='epoch',\n",
    ")\n",
    "writer_log = tf.summary.create_file_writer(path_logs)\n",
    "\n",
    "# Build dataset\n",
    "tfds, ds = build_dataset(\n",
    "    root, batch_size=batch_size, use_template=False, shuffle=True\n",
    ")\n",
    "\n",
    "# Train/validation split (80/20)\n",
    "index_split = int(0.8 * (len(ds) // batch_size))\n",
    "dataset_train = tfds.take(index_split).prefetch(AUTOTUNE)\n",
    "dataset_valid = tfds.skip(index_split).prefetch(AUTOTUNE)\n",
    "\n",
    "# Initialize model\n",
    "model = build_model(ds.num_classes, shape=ds.shape, learning_rate=5e-5)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Training Execution\n",
    "\n",
    " Train the 3D ResNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    dataset_train.repeat(),\n",
    "    validation_data=dataset_valid.repeat(),\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=18,  # Number of batches per epoch\n",
    "    validation_steps=18,  # Validation batches per epoch\n",
    "    callbacks=[callback_tensorboard],\n",
    ")\n",
    "\n",
    "# Save trained model\n",
    "model.save(osp.join(path_logs, 'antspynet_3d_classifier.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Model Interpretation\n",
    "\n",
    " Visualize model decisions using Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def make_gradcam_heatmap(\n",
    "    volume: tf.Tensor,\n",
    "    model: tf.keras.Model,\n",
    "    last_conv_layer_name: str,\n",
    "    pred_index: int = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate Grad-CAM heatmap for 3D volume\n",
    "\n",
    "    Args:\n",
    "        volume: Input volume (batch_size, D, H, W, C)\n",
    "        model: Trained Keras model\n",
    "        last_conv_layer_name: Name of last convolutional layer\n",
    "        pred_index: Class index to generate heatmap for (default: predicted class)\n",
    "\n",
    "    Returns:\n",
    "        heatmap: 3D activation heatmap\n",
    "    \"\"\"\n",
    "    # Create gradient model\n",
    "    grad_model = Model(\n",
    "        [model.inputs],\n",
    "        [model.get_layer(last_conv_layer_name).output, model.output],\n",
    "    )\n",
    "\n",
    "    # Compute gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(volume)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "\n",
    "    # Compute guided gradients\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2, 3))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # Normalize heatmap\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap + 1e-8)\n",
    "    heatmap = tf.image.resize(\n",
    "        heatmap[..., tf.newaxis], (volume.shape[1], volume.shape[2])\n",
    "    )\n",
    "    return heatmap.numpy()\n",
    "\n",
    "\n",
    "def display_heatmap(\n",
    "    volume: np.ndarray,\n",
    "    heatmap: np.ndarray,\n",
    "    alpha: float = 0.5,\n",
    "    slice_axis: int = -1,\n",
    "    slice_index: int = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Display heatmap overlay on MRI slice\n",
    "\n",
    "    Args:\n",
    "        volume: Original MRI volume\n",
    "        heatmap: Activation heatmap\n",
    "        alpha: Heatmap opacity\n",
    "        slice_axis: Axis to slice along (0=axial, 1=coronal, 2=sagittal)\n",
    "        slice_index: Slice index to display\n",
    "    \"\"\"\n",
    "    if volume.ndim == 4:\n",
    "        volume = volume[..., 0]\n",
    "    if heatmap.ndim == 4:\n",
    "        heatmap = heatmap[..., 0]\n",
    "\n",
    "    # Default to middle slice\n",
    "    if slice_index is None:\n",
    "        slice_index = volume.shape[slice_axis] // 2\n",
    "\n",
    "    # Extract slice\n",
    "    if slice_axis == 0:\n",
    "        v_slice = volume[slice_index, :, :]\n",
    "        h_slice = heatmap[slice_index, :, :]\n",
    "    elif slice_axis == 1:\n",
    "        v_slice = volume[:, slice_index, :]\n",
    "        h_slice = heatmap[:, slice_index, :]\n",
    "    else:\n",
    "        v_slice = volume[:, :, slice_index]\n",
    "        h_slice = heatmap[:, :, slice_index]\n",
    "\n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(v_slice, cmap='gray', interpolation='none')\n",
    "    plt.imshow(h_slice, cmap='jet', alpha=alpha, interpolation='none')\n",
    "    plt.title(f\"Grad-CAM Overlay (axis={slice_axis}, index={slice_index})\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Generate and Visualize Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify last convolutional layer\n",
    "names_conv3d = [\n",
    "    l.name for l in model.layers if isinstance(l, tf.keras.layers.Conv3D)\n",
    "]\n",
    "print(\"Convolutional layers:\", names_conv3d)\n",
    "\n",
    "# Generate heatmap for validation sample\n",
    "for sample in dataset_valid.take(1):\n",
    "    volume, label = sample\n",
    "    volume = tf.tile(\n",
    "        volume[:1], [2, 1, 1, 1, 1]\n",
    "    )  # Duplicate for batch processing\n",
    "    heatmap = make_gradcam_heatmap(\n",
    "        volume, model, last_conv_layer_name=names_conv3d[-1]\n",
    "    )\n",
    "\n",
    "    # Display 2D slice with heatmap\n",
    "    idx_slice = volume.shape[1] // 2\n",
    "    mid_slice = volume[0, idx_slice, :, :, :].numpy()\n",
    "    display_heatmap(mid_slice, heatmap[..., 0], slice_axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Interactive 3D Heatmap Visualization\n",
    "\n",
    " Requires Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_heatmap_3d(\n",
    "    volume: np.ndarray,\n",
    "    heatmap: np.ndarray,\n",
    "    threshold: float = 0.6,\n",
    "    downsample_factor: int = 8,\n",
    "):\n",
    "    \"\"\"\n",
    "    Render 3D heatmap using Plotly\n",
    "\n",
    "    Args:\n",
    "        volume: Original MRI volume\n",
    "        heatmap: 3D activation heatmap\n",
    "        threshold: Minimum activation value to display\n",
    "        downsample_factor: Downsampling factor for visualization\n",
    "    \"\"\"\n",
    "    # Preprocess volume\n",
    "    if volume.ndim == 5:\n",
    "        volume = volume[0, :, :, :, 0]\n",
    "    volume = np.rot90(volume, 1, axes=(1, 2))\n",
    "    coords = downsample(get_surface_coords(volume > 0), 60)\n",
    "\n",
    "    # Create brain surface plot\n",
    "    surface = go.Scatter3d(\n",
    "        x=coords[:, 0],\n",
    "        y=coords[:, 1],\n",
    "        z=coords[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=2, color='gray'),\n",
    "        name='Brain',\n",
    "        hoverinfo='skip',\n",
    "    )\n",
    "\n",
    "    # Prepare heatmap data\n",
    "    if heatmap.ndim == 4:\n",
    "        heatmap = heatmap[..., 0]\n",
    "    pad = np.zeros_like(volume)\n",
    "    indices = np.linspace(0, volume.shape[0] - 1, heatmap.shape[0], dtype=int)\n",
    "    pad[indices, :, :] = heatmap\n",
    "    coords = np.argwhere(pad >= threshold)\n",
    "    if downsample_factor > 1:\n",
    "        coords = downsample(coords, downsample_factor)\n",
    "    values = pad[tuple(coords.T)]\n",
    "\n",
    "    # Create heatmap plot\n",
    "    heatmaps = go.Scatter3d(\n",
    "        x=coords[:, 0],\n",
    "        y=coords[:, 1],\n",
    "        z=coords[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=2, color=values, colorscale='jet', opacity=0.5),\n",
    "        name='Activation',\n",
    "        hoverinfo='skip',\n",
    "    )\n",
    "\n",
    "    # Configure layout\n",
    "    layout = go.Layout(\n",
    "        title='3D Heatmap (Grad-CAM)',\n",
    "        scene=dict(\n",
    "            xaxis=dict(visible=False),\n",
    "            yaxis=dict(visible=False),\n",
    "            zaxis=dict(visible=False),\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=40),\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[heatmaps, surface], layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Visualize 3D heatmap\n",
    "render_heatmap_3d(volume.numpy(), heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Training Metrics Visualization\n",
    "\n",
    " Load and display training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing.event_accumulator import (\n",
    "    EventAccumulator,\n",
    ")\n",
    "\n",
    "\n",
    "def display_logs(logdir: str = 'runs'):\n",
    "    \"\"\"\n",
    "    Display training metrics from TensorBoard logs\n",
    "\n",
    "    Args:\n",
    "        logdir: Root directory containing log files\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_scalar_from_tensor(tensor_proto):\n",
    "        t = tf.make_ndarray(tensor_proto)\n",
    "        return float(t) if t.size == 1 else None\n",
    "\n",
    "    def extract_events(path):\n",
    "        files = glob(osp.join(path, '*tfevents*'))\n",
    "        if not files:\n",
    "            return None\n",
    "        event = EventAccumulator(files[-1])\n",
    "        event.Reload()\n",
    "        events = {}\n",
    "\n",
    "        # Process scalar events\n",
    "        for tag in event.Tags().get('scalars', []):\n",
    "            events[tag] = [(x.step, x.value) for x in event.Scalars(tag)]\n",
    "\n",
    "        # Process tensor events\n",
    "        for tag in event.Tags().get('tensors', []):\n",
    "            if not tag.startswith('epoch'):\n",
    "                continue\n",
    "            entries = event.Tensors(tag)\n",
    "            values = []\n",
    "            for e in entries:\n",
    "                val = extract_scalar_from_tensor(e.tensor_proto)\n",
    "                if val is not None:\n",
    "                    values.append((e.step, val))\n",
    "            if values:\n",
    "                events[tag] = values\n",
    "        return events\n",
    "\n",
    "    # Process all subdirectories\n",
    "    for subdir in ['train', 'validation', '.']:\n",
    "        path = osp.join(logdir, subdir)\n",
    "        if not osp.isdir(path):\n",
    "            continue\n",
    "\n",
    "        logs = extract_events(path)\n",
    "        if not logs:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{subdir.upper()} LOGS\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot each metric\n",
    "        for tag, points in logs.items():\n",
    "            steps, values = zip(*points)\n",
    "            plt.plot(steps, values, label=tag)\n",
    "\n",
    "        plt.title(f\"{subdir.title()} Metrics\")\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Display training logs\n",
    "display_logs(path_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous\n",
    "\n",
    "Print out all loaded Python modules and their dependencies (for environment pinning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pkg_resources\n",
    "import subprocess\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Get a list of all imported modules\n",
    "imported_modules = {\n",
    "    name: module\n",
    "    for name, module in sys.modules.items()\n",
    "    if module and getattr(module, '__file__', None)\n",
    "}\n",
    "\n",
    "# Get the version and dependencies of each imported module\n",
    "package_info = defaultdict(dict)\n",
    "\n",
    "for name in imported_modules:\n",
    "    try:\n",
    "        # Skip non-package modules\n",
    "        if name.startswith('_') or name in sys.builtin_module_names:\n",
    "            continue\n",
    "\n",
    "        dist = pkg_resources.get_distribution(name)\n",
    "        version = dist.version\n",
    "        package_info[name]['version'] = version\n",
    "\n",
    "        # Get dependencies using pip\n",
    "        result = subprocess.run(\n",
    "            ['pip', 'show', name], capture_output=True, text=True\n",
    "        )\n",
    "        dependencies = []\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if line.startswith('Requires: '):\n",
    "                dependencies = [\n",
    "                    d.strip()\n",
    "                    for d in line.split('Requires: ')[1].split(',')\n",
    "                    if d.strip()\n",
    "                ]\n",
    "                break\n",
    "\n",
    "        package_info[name]['dependencies'] = dependencies\n",
    "    except (pkg_resources.DistributionNotFound, subprocess.CalledProcessError):\n",
    "        pass\n",
    "\n",
    "# Print the package information\n",
    "for name, info in package_info.items():\n",
    "    print(f\"Package: {name}\")\n",
    "    print(f\"Version: {info.get('version', 'N/A')}\")\n",
    "    print(f\"Dependencies: {', '.join(info.get('dependencies', []))}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
